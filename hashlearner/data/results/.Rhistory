shiny::runApp('Documents/b-statistics/APP')
runApp('Documents/b-statistics/APP')
runApp('Documents/b-statistics/APP')
runApp('Documents/b-statistics/APP')
runApp('Documents/b-statistics/APP')
runApp('Documents/b-statistics/APP')
runApp('Documents/b-statistics/APP')
runApp('Documents/b-statistics/APP')
runApp('Documents/b-statistics/APP')
pchisq(q=100, df=80, ncp = 0, lower.tail = FALSE, log.p = FALSE)
pi
area =  function(r){
return(pi*r^2)
}
area(20)
?rnorm()
radi<-rnorm(n = 10000, mean = 20,sd=2)
radi
areas<-sapply(radi,area)
mean(area)
area
area(20)
area =  function(r){
return(pi*r^2)
}
area(20)
areas<-sapply(radi,area)
mean(area)
areas<-sapply(radi,area)
areas
areas<-sapply(radi,area)
mean(area)
mean(1)
mean(as.numeric(area))
area
mean(areas)
area(20)
var(areas)
?var
areas
sd(areas)
area(20)
mean(areas)
areas<-sapply(radi,area)
areas
mean(areas)
area(20)
sqrt(sd(areas))
area(20)
mean(areas)-sqrt(sd(areas))
area(20)
radi
hist(radi)
mean(areas)
mean(areas)-4
area(20)
sqrt(sd(areas))
mean(areas)-sqrt(sd(areas))
#Actual Area
area(20)
radi<-rnorm(n = 10000, mean = 20,sd=2)
radi
var(radi)
radi<-rnorm(n = 10000, mean = 20,sd=2)
areas<-sapply(radi,area)
mean(areas)-16
#Actual Area
area(20)
mean(areas)-16
#Actual Area
area(20)
area =  function(r){
return(pi*r^2)
}
radi<-rnorm(n = 10000, mean = 20,sd=2)
area(20)
radi<-rnorm(n = 10000, mean = 20,sd=2)
areas<-sapply(radi,area)
mean(areas)
area(20)
mean(areas)-area(20)
mean(areas)-area(20)
radi<-rnorm(n = 10000, mean = 20,sd=2)
areas<-sapply(radi,area)
mean(areas)
#Actual Area
area(20)
mean(areas)-area(20)
radi<-rnorm(n = 10000, mean = 20,sd=2)
areas<-sapply(radi,area)
mean(areas)
#Actual Area
area(20)
mean(areas)-area(20)
radi<-rnorm(n = 10000, mean = 20,sd=2)
areas<-sapply(radi,area)
mean(areas)
#Actual Area
area(20)
mean(areas)-area(20)
radi<-rnorm(n = 10000, mean = 20,sd=2)
areas<-sapply(radi,area)
mean(areas)
#Actual Area
area(20)
mean(areas)-area(20)
radi<-rnorm(n = 1000000, mean = 20,sd=2)
areas<-sapply(radi,area)
mean(areas)
#Actual Area
area(20)
mean(areas)-area(20)
radi<-rnorm(n = 1000000, mean = 20,sd=2)
areas<-sapply(radi,area)
mean(areas)
#Actual Area
area(20)
mean(areas)-area(20)
radi<-rnorm(n = 10000, mean = 20,sd=2)
radi<-rnorm(n = 100000, mean = 20,sd=2)
areas<-sapply(radi,area)
area(20)
mean(areas)-pi*area(20)
mean(areas)-pi*4
area(20)
mean(areas)-pi*4
#1256.363
?par
par(mfrow = c(2,2))
?rgamma
?rchisq
?rf
hist(rnorm(10000,0,1))
hist(rgamma(10000,0,1))
rchisq(rnorm(10000,1))
hist(rf(10000,1,2
hist(rgamma(10000,1,1))
rchisq(rnorm(10000,1))
hist(rf(10000,1,2
hist(rnorm(10000,0,1))
hist(rf(10000,1,2 ))
hist(rf(10000,4,2 ))
hist(rf(100,5,2 ))
qf(100,5,2 )
hist(rf(100,5,2 ))
hist(rf(100,5,5 ))
rchisq(rnorm(10000,1))
rchisq(rchisq(10000,1))
hist(rchisq(10000,1))
hist(rgamma(10000,1,1))
hist(rnorm(10000,0,1))
par(mfrow = c(2,2))
hist(rnorm(10000,0,1))
hist(rgamma(10000,1,1))
hist(rchisq(10000,1))
hist(rf(100,5,5 ))
par(mfrow = c(2,2))
hist(rnorm(10000,0,1))
hist(rgamma(10000,1,1))
hist(rchisq(10000,1))
hist(rf(100,5,5 ))
pchisq(q=100, df=80, ncp = 0, lower.tail = FALSE, log.p = FALSE)
library(keras)
library(magrittr)
library(data.table)
###Cleaning###
sentiment_df<-fread("data/sentiment_raw.csv")
sentiment_df<-sentiment_df[1:10000,]
sentiment_df$ascii<-iconv(sentiment_df$JustText, "latin1", "ASCII", sub="")
sentiment_df$no_repeats<-gsub("([[:alpha:]])\\1{2,}", "\\1", sentiment_df$ascii)
sentiment_df$no_repeats<-gsub("(?<=[\\s])\\s*|^\\s+|\\s+$","",perl=TRUE,sentiment_df$no_repeats)
sentiment_df<-sentiment_df[sapply(gregexpr("\\W+", sentiment_df$no_repeats), length) >1,]
sentiment_df_small<-sentiment_df
load("~/Documents/MAS/418/Hw4/data/cleaning_2.RData")
set.seed(123)
training_index<-sample(1:length(nn_predictors),length(nn_predictors)*.75)
x_train<- nn_predictors
x_test <- nn_predictors[-training_index]
y_train<- sentiment_df_small$Sentiment
y_test <- sentiment_df_small$Sentiment[-training_index]
### Test and Train ###
max_features <- length(factor_list)
maxlen <- 80  # cut texts after this number of words
batch_size <- 32
print('Loading data...\n')
cat(length(x_train), 'train sequences\n')
cat(length(x_test), 'test sequences\n')
print('Pad sequences (samples x time)\n')
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
cat('x_train shape:', dim(x_train), '\n')
cat('x_test shape:', dim(x_test), '\n')
print('Build model...\n')
model <- keras_model_sequential()
model %>%
layer_embedding(input_dim = max_features, output_dim = 128) %>%
layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>%
layer_dense(units = 1, activation = 'sigmoid')
# try using different optimizers and different optimizer configs
model %>% compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
print('Train...\n')
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = 1,
validation_data = list(x_test, y_test)
)
ggplot
library(ggplot)
install.packages("ggplot")
library(ggplot2)
ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Phase)) +
geom_bar(stat="identity") + coord_flip()
ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Phase)) +
geom_bar(stat="identity") + coord_flip()
names(iris)
names(mtcars)
view(mtcars)
View(mtcars)
ggplot(mtcars, aes(x=carb, y=mpg, fill=gear))
ggplot(mtcars, aes(x=carb, y=mpg, fill=gear)) +
geom_bar(stat="identity") + coord_flip()
library(mgcv)
qplot(carat, price, data = dsmall, geom = c("point", "smooth"),method = "gam", formula = y ~ s(x))
dsmall
qplot(mpg, disp, data = mtcars, geom = c("point", "smooth"),method = "gam", formula = y ~ s(x))
qplot(mpg, disp, data = mtcars, geom = c("point", "smooth"), formula = y ~ s(x))
qplot(mpg, disp, data = mtcars, geom = c("point", "smooth"))
View(AirPassengers)
austres
LakeHuron
qplot( data = LakeHuron, geom = c("point", "smooth"))
View(LakeHuron)
View(as.data.frame(LakeHuron))
str(LakeHuron)
time(LakeHuron)
lake = data.frame(LakeHuron,time(LakeHuron)
)
names(lake)
?data.frame
lake = data.frame(LakeHuron,time(LakeHuron),names = c("level","time"))
names(lake)
names(lake) = c("level","time")
library(mgcv)
lake = data.frame(LakeHuron,time(LakeHuron))
names(lake) = c("level","time")
qplot(level, time, data = lake, geom = c("point", "smooth"))
qplot(time, level, data = lake, geom = c("point", "smooth"))
as.numeric(LakeHuron)
lake = data.frame(as.numeric(LakeHuron),time(LakeHuron))
names(lake) = c("level","time")
qplot(time, level, data = lake, geom = c("point", "smooth"))
lake = data.frame(LakeHuron,as.numeric(time(LakeHuron)))
names(lake) = c("level","time")
qplot(time, level, data = lake, geom = c("point", "smooth"))
lake = data.frame(as.numeric(LakeHuron),as.numeric(time(LakeHuron)))
names(lake) = c("level","time")
qplot(time, level, data = lake, geom = c("point", "smooth"))
as.numeric(LakeHuron)
as.numeric(time(LakeHuron))
lake = data.frame(as.numeric(LakeHuron)[1:10],as.numeric(time(LakeHuron))[1:10])
lake
lake = data.frame(as.numeric(LakeHuron)[1:10],as.numeric(time(LakeHuron))[1:10])
names(lake) = c("level","time")
qplot(time, level, data = lake, geom = c("point", "smooth"))
n <- 10
d <- data.frame(x = 1:n, y = rnorm(n))
ggplot(d,aes(x,y)) + geom_point() +
geom_line(data=data.frame(spline(d, n=n*10)))
aes
?aes
ggplot(d,aes(x,y))
ggplot(d,aes(x,y)) + geom_point()
geom_line(data=data.frame(spline(d)))
geom_line(data=data.frame(spline(d, n=n)))
geom_line(data=data.frame(spline(d, n=n*10)))
ggplot(d,aes(x,y)) + geom_point() +
geom_line(data=data.frame(spline(d))
)
ggplot(d,aes(x,y)) + geom_point() +
geom_line(data=data.frame(spline(d,n=n)))
ggplot(d,aes(x,y)) + geom_point() +
geom_line(data=data.frame(spline(d,n=n*10)))
install.packages(c("bindrcpp", "boot", "cluster", "codetools", "coin", "colorspace", "curl", "DBI", "deldir", "devtools", "digest", "dplyr", "evaluate", "foreign", "formatR", "ggplot2", "git2r", "glmnet", "glue", "goftest", "h2o", "htmltools", "htmlwidgets", "httpuv", "httr", "keras", "knitr", "lattice", "lme4", "lubridate", "maps", "markdown", "MASS", "Matrix", "mgcv", "mime", "mvtnorm", "nlme", "openssl", "packrat", "party", "plotly", "polyclip", "pROC", "purrr", "R6", "randomForestSRC", "Rcpp", "RcppEigen", "reshape", "reshape2", "reticulate", "rlang", "rmarkdown", "RMySQL", "rpart", "rsconnect", "RSQLite", "rstudioapi", "sandwich", "scales", "shiny", "sp", "spatstat", "splancs", "sqldf", "stringi", "stringr", "survival", "tensorflow", "tibble", "tidyr", "viridis", "withr", "XML", "zoo"))
install.packages(c("bindrcpp", "boot", "cluster", "codetools",
setwd("~/Documents/MAS/thesis/Reinforcement-Learning-Study/hashlearner/data/results")
library(magrittr)
require(randomForest)
library(tidyverse)
library(JOUSBoost)
library(maboost)
library(gbm)
library(onehot)
library(xgboost)
library(readr)
library(stringr)
library(caret)
library(car)
EXPECTED_NAMES = c("expected.0","expected.1", "expected.2" ,"expected.3" ,"expected.4", "expected.5", "expected.6" ,"expected.7" ,"expected.8", "expected.9")
get_files = function(folder){
files <- list.files(path = folder, pattern = "*.csv")
files <- paste(folder,files,sep="")
myfiles = lapply(files, read.csv)
return(myfiles)
}
get_data_from_folder = function(split_ratio, folder, first_half){
csv_files = get_files(folder)
predictions = lapply(csv_files,function(df) df$predicted)
predictions_df = as.data.frame(do.call(cbind,predictions))
expected = csv_files[[1]]$expected
total_df = cbind(expected,predictions_df)
total_df = data.frame(lapply(X=total_df,FUN = as.factor),stringsAsFactors=FALSE)
split_index = round(nrow(total_df)*split_ratio)
if(first_half)
return(total_df[1:split_index,])
else
return(total_df[-1:-split_index,])
}
get_binary_data_from_folder = function(split_ratio, folder, first_half){
total_df = get_data_from_folder(split_ratio, folder, first_half)
encoder <- onehot(total_df[-1])
one_hot_df <- data.frame(predict(encoder, total_df))
one_hot_df$expected = total_df$expected
return(one_hot_df)
}
get_binary_data_total = function(split_ratio, folder, first_half){
total_df = get_data_from_folder(split_ratio, folder, first_half)
encoder <- onehot(total_df)
one_hot_df <- data.frame(predict(encoder, total_df))
return(one_hot_df)
}
train_rf = function(training, ntrees = 100){
trained_rf=randomForest(expected ~ . , data = training, ntrees=ntrees)
return(trained_rf)
}
test_rf = function(testing, trained_rf){
predicted = predict(trained_rf,testing)
return(predicted)
}
get_performance = function(predicted, expected){
results = table(predicted,expected)
successful_predictions = sum(diag(results))
total_predictions = length(predicted)
success_rate = successful_predictions/total_predictions
print(paste("Succes Rate:",success_rate))
}
get_all_performance = function (testing){
print("getting all initial performance")
expected = testing$expected
for(predicted in testing[-1]){
get_performance(predicted,expected)
}
}
xg_boost_train = function(training,nround = 10){
raw_training = data.matrix(training[names(training) != EXPECTED_NAMES])
xg_list = NULL
i = 1
for(expected_name in EXPECTED_NAMES){
label = as.numeric(unlist(training[expected_name]))
bstSparse <- xgboost(data = raw_training, label = label, max.depth = 5, eta = 1, nthread = 2, nround = nround, objective = "binary:logistic")
xg_list[i] = list(bstSparse)
i=i+1
}
xg_list
}
xgb_predict = function(testing,xg_list){
extract_prediction = function(testing_xgb,raw_testing){
gbm_predicted <- predict(testing_xgb, raw_testing)
#gbm_predicted <- plogis(2*gbm_predicted)
return(unlist(gbm_predicted))
}
raw_testing = data.matrix(testing[names(training) != EXPECTED_NAMES])
probs = lapply(xg_list, FUN=extract_prediction,raw_testing=raw_testing)
probs_df = as.data.frame(do.call(cbind,probs))
apply(probs_df,MARGIN = 2,FUN=sd)
probs_df=scale(probs_df, scale=TRUE)
colMeans(probs_df)
predicted = apply(MARGIN = 1, probs_df, FUN = function(row_df) which(row_df == max(row_df))[[1]])-1
predicted
}
main = function(){
folder = "./analysis_results_1-50000-60000/"
training = get_binary_data_total(split_ratio = .5, folder = folder, first_half = TRUE)
testing = get_binary_data_total(split_ratio = .5, folder = folder, first_half = FALSE)
expected = get_data_from_folder(split_ratio = .5, folder = folder, first_half = FALSE)$expected
trained_xg_list = xg_boost_train(training, nround = 8)
predicted = xgb_predict(testing, trained_xg_list)
#get_all_performance(testing)
get_performance(predicted, expected)
}
main()
folder = "./analysis_results_1-50000-60000/"
training = get_binary_data_total(split_ratio = .5, folder = folder, first_half = TRUE)
testing = get_binary_data_total(split_ratio = .5, folder = folder, first_half = FALSE)
expected = get_data_from_folder(split_ratio = .5, folder = folder, first_half = FALSE)$expected
trained_xg_list = xg_boost_train(training, nround = 8)
predicted = xgb_predict(testing, trained_xg_list)
#get_all_performance(testing)
get_performance(predicted, expected)
1-0.964
xg_list = xg_boost_train(training, nround = 8)
predicted = xgb_predict(testing, xg_list)
raw_testing = data.matrix(testing[names(training) != EXPECTED_NAMES])
probs = lapply(xg_list, FUN=extract_prediction,raw_testing=raw_testing)
probs_df = as.data.frame(do.call(cbind,probs))
apply(probs_df,MARGIN = 2,FUN=sd)
probs_df=scale(probs_df, scale=TRUE)
extract_prediction = function(testing_xgb,raw_testing){
gbm_predicted <- predict(testing_xgb, raw_testing)
#gbm_predicted <- plogis(2*gbm_predicted)
return(unlist(gbm_predicted))
}
raw_testing = data.matrix(testing[names(training) != EXPECTED_NAMES])
probs = lapply(xg_list, FUN=extract_prediction,raw_testing=raw_testing)
probs_df = as.data.frame(do.call(cbind,probs))
apply(probs_df,MARGIN = 2,FUN=sd)
probs_df=scale(probs_df, scale=TRUE)
colMeans(probs_df)
predicted = apply(MARGIN = 1, probs_df, FUN = function(row_df) which(row_df == max(row_df))[[1]])-1
predicted
get_performance
results = table(predicted,expected)
results
